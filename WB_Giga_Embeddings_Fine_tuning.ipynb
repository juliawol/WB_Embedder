{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYpvR75UwbyxCMmF1e+08R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliawol/WB_Embedder/blob/main/WB_Giga_Embeddings_Fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from sklearn.metrics import ndcg_score, accuracy_score, f1_score\n",
        "\n",
        "MODEL_NAME = \"ai-sage/Giga-Embeddings-instruct\"\n",
        "OUTPUT_DIR = \"./fine_tuned_giga_embeddings\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "DATASETS = {\n",
        "    \"wbqasupport_facts\": \"wbqasupportfacts.csv\",\n",
        "    \"wbqasupport_short\": \"wbqasupportshort.csv\",\n",
        "    \"wbsearch_query_nm\": \"regenerated_wbsearchquerynmrelevance.csv\",\n",
        "    \"wbreviews\": \"regenerated_wbreviews.csv\",\n",
        "    \"wbgoods_categories\": \"regenerated_wbgoodscategoriesclassification.csv\",\n",
        "    \"wbsim_goods\": \"regenerated_wbsimgoodstriplets.csv\",\n",
        "    \"wbgender_classification\": \"regenerated_wbgenderclassification.csv\",\n",
        "    \"wbbrand_syns\": \"synthetic_wb_brand_syns.csv\"\n",
        "}\n",
        "\n",
        "# Load each dataset into a DatasetDict\n",
        "def preprocess_data(file_path, task):\n",
        "    data = load_dataset(\"csv\", data_files=file_path)\n",
        "\n",
        "    if task == \"retrieval\":\n",
        "        def tokenize_retrieval(example):\n",
        "            if \"context\" in example and \"question\" in example:\n",
        "                return tokenizer(example[\"question\"], example[\"context\"], truncation=True)\n",
        "            elif \"previous_product\" in example and \"next_product\" in example:\n",
        "                return tokenizer(example[\"previous_product\"], example[\"next_product\"], truncation=True)\n",
        "\n",
        "        data = data.map(tokenize_retrieval, batched=True)\n",
        "\n",
        "    elif task == \"classification\":\n",
        "        def tokenize_classification(example):\n",
        "            if \"review_text\" in example:\n",
        "                return tokenizer(example[\"review_text\"], truncation=True)\n",
        "            elif \"product_name\" in example:\n",
        "                return tokenizer(example[\"product_name\"], truncation=True)\n",
        "\n",
        "        data = data.map(tokenize_classification, batched=True)\n",
        "\n",
        "    elif task == \"pairwise\":\n",
        "        def tokenize_pairwise(example):\n",
        "            return tokenizer(example[\"brand_name\"], example[\"brand_synonym\"], truncation=True)\n",
        "\n",
        "        data = data.map(tokenize_pairwise, batched=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Prepare datasets by tasks\n",
        "data_by_task = {\n",
        "    \"retrieval\": [\"wbqasupport_facts\", \"wbqasupport_short\", \"wbsearch_query_nm\", \"wbsim_goods\"],\n",
        "    \"classification\": [\"wbreviews\", \"wbgoods_categories\", \"wbgender_classification\"],\n",
        "    \"pairwise\": [\"wbbrand_syns\"]\n",
        "}\n",
        "\n",
        "prepared_datasets = {}\n",
        "for task, dataset_names in data_by_task.items():\n",
        "    prepared_datasets[task] = DatasetDict()\n",
        "    for name in dataset_names:\n",
        "        prepared_datasets[task][name] = preprocess_data(DATASETS[name], task=task)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=2,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False\n",
        ")\n",
        "\n",
        "# Define a Trainer for each group\n",
        "def train_model(task, dataset_dict):\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        predictions = torch.argmax(torch.tensor(logits), dim=-1).numpy()\n",
        "        labels = labels.numpy()\n",
        "\n",
        "        if task == \"retrieval\":\n",
        "            # Compute NDCG metrics\n",
        "            ndcg1 = ndcg_score([labels], [predictions], k=1)\n",
        "            ndcg10 = ndcg_score([labels], [predictions], k=10)\n",
        "            return {\"NDCG@1\": ndcg1, \"NDCG@10\": ndcg10}\n",
        "        elif task == \"classification\":\n",
        "            # Compute Accuracy and F1 metrics\n",
        "            accuracy = accuracy_score(labels, predictions)\n",
        "            f1 = f1_score(labels, predictions, average=\"weighted\")\n",
        "            return {\"accuracy\": accuracy, \"f1\": f1}\n",
        "        elif task == \"pairwise\":\n",
        "            # Compute NDCG metrics for pairwise tasks\n",
        "            ndcg1 = ndcg_score([labels], [predictions], k=1)\n",
        "            ndcg10 = ndcg_score([labels], [predictions], k=10)\n",
        "            return {\"NDCG@1\": ndcg1, \"NDCG@10\": ndcg10}\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset_dict[list(dataset_dict.keys())[0]]['train'],\n",
        "        eval_dataset=dataset_dict[list(dataset_dict.keys())[0]]['validation'],\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "    trainer.train()\n",
        "    trainer.save_model(os.path.join(OUTPUT_DIR, task))\n",
        "\n",
        "# Train models for grouped tasks\n",
        "for task, dataset_dict in prepared_datasets.items():\n",
        "    train_model(task, dataset_dict)\n",
        "\n",
        "print(\"Fine-tuning completed and models saved.\")\n"
      ],
      "metadata": {
        "id": "4t3evi2SHnhZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}