{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaHf42lzLNx7pJiDYVIu5h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliawol/WB_Embedder/blob/main/WB_Embedder_GigaChat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6JDmv-Q3CSa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset, Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = \"ai-sage/Giga-Embeddings-instruct\"\n",
        "BATCH_SIZE = 32\n",
        "MAX_LENGTH = 256\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 1e-5\n",
        "WARMUP_STEPS = 500\n",
        "RANDOM_SEED = 42\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Dataset paths\n",
        "CARDS_DATASET = \"JuliaWolken/WB_CARDS\"\n",
        "TRIPLETS_DATASET = \"JuliaWolken/WB_TRIPLETS\"\n",
        "BRANDS_DATASET = \"JuliaWolken/WB_BRANDS\"\n",
        "\n",
        "# Instructional prefixes\n",
        "task_name_to_instruct = {\n",
        "    \"retrieval\": \"Дано вопрос, необходимо получить наиболее релевантный товар\\nquestion: \",\n",
        "    \"ranking\": \"Rank items based on their relevance\",\n",
        "    \"classification\": \"Classify the given input into predefined categories\",\n",
        "}\n",
        "query_prefix = task_name_to_instruct[\"retrieval\"]\n",
        "passage_prefix = \"\"\n",
        "\n",
        "# Set random seed\n",
        "random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# Data Loading\n",
        "print(\"Loading datasets...\")\n",
        "data_sampled = load_dataset(CARDS_DATASET)[\"train\"]\n",
        "triplet_candidates = load_dataset(TRIPLETS_DATASET)[\"train\"]\n",
        "brand_candidates = load_dataset(BRANDS_DATASET)[\"train\"]\n",
        "\n",
        "data_sampled_df = data_sampled.to_pandas()\n",
        "triplet_candidates_df = triplet_candidates.to_pandas()\n",
        "brand_candidates_df = brand_candidates.to_pandas()\n",
        "\n",
        "\n",
        "# Dataset Class and Loaders\n",
        "class TripletDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, query_prefix, passage_prefix, max_length=256):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.query_prefix = query_prefix\n",
        "        self.passage_prefix = passage_prefix\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        query, positive, negative = row[\"Anchor\"], row[\"Positive\"], row[\"Negative\"]\n",
        "\n",
        "        query_text = self.query_prefix + query\n",
        "        positive_text = self.passage_prefix + positive\n",
        "        negative_text = self.passage_prefix + negative\n",
        "\n",
        "        query_enc = self.tokenizer(query_text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
        "        positive_enc = self.tokenizer(positive_text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
        "        negative_enc = self.tokenizer(negative_text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
        "\n",
        "        return {\n",
        "            \"query_input_ids\": query_enc[\"input_ids\"].squeeze(0),\n",
        "            \"query_attention_mask\": query_enc[\"attention_mask\"].squeeze(0),\n",
        "            \"positive_input_ids\": positive_enc[\"input_ids\"].squeeze(0),\n",
        "            \"positive_attention_mask\": positive_enc[\"attention_mask\"].squeeze(0),\n",
        "            \"negative_input_ids\": negative_enc[\"input_ids\"].squeeze(0),\n",
        "            \"negative_attention_mask\": negative_enc[\"attention_mask\"].squeeze(0),\n",
        "        }\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Triplet loader\n",
        "triplet_dataset = TripletDataset(triplet_candidates_df, tokenizer, query_prefix, passage_prefix)\n",
        "triplet_loader = DataLoader(triplet_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Model Definition\n",
        "class MultiTaskModel(nn.Module):\n",
        "    def __init__(self, model_name, num_classes=60):\n",
        "        super(MultiTaskModel, self).__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
        "        self.classification_head = nn.Linear(self.encoder.config.hidden_size, num_classes)  # Classification tasks\n",
        "        self.ranking_head = nn.Linear(self.encoder.config.hidden_size, 1)  # Ranking tasks\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, task=\"classification\"):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_emb = outputs.last_hidden_state[:, 0, :]\n",
        "        if task == \"classification\":\n",
        "            return self.classification_head(cls_emb)\n",
        "        elif task == \"ranking\":\n",
        "            return self.ranking_head(cls_emb)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown task\")\n",
        "\n",
        "# Initialize model\n",
        "model = MultiTaskModel(MODEL_NAME).to(DEVICE)\n",
        "\n",
        "# Optimization and Loss\n",
        "def contrastive_loss(query_emb, positive_emb, negative_emb, margin=0.2):\n",
        "    return F.triplet_margin_loss(query_emb, positive_emb, negative_emb, margin=margin)\n",
        "\n",
        "classification_criterion = nn.CrossEntropyLoss()  # For classification tasks\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=(len(triplet_loader)) * EPOCHS)\n",
        "\n",
        "\n",
        "# Training Loop\n",
        "print(\"Starting fine-tuning...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for step, batch in enumerate(triplet_loader):\n",
        "        query_input_ids = batch[\"query_input_ids\"].to(DEVICE)\n",
        "        query_attention_mask = batch[\"query_attention_mask\"].to(DEVICE)\n",
        "        positive_input_ids = batch[\"positive_input_ids\"].to(DEVICE)\n",
        "        positive_attention_mask = batch[\"positive_attention_mask\"].to(DEVICE)\n",
        "        negative_input_ids = batch[\"negative_input_ids\"].to(DEVICE)\n",
        "        negative_attention_mask = batch[\"negative_attention_mask\"].to(DEVICE)\n",
        "\n",
        "        # Encode embeddings\n",
        "        query_emb = model.encoder(input_ids=query_input_ids, attention_mask=query_attention_mask).last_hidden_state[:, 0, :]\n",
        "        positive_emb = model.encoder(input_ids=positive_input_ids, attention_mask=positive_attention_mask).last_hidden_state[:, 0, :]\n",
        "        negative_emb = model.encoder(input_ids=negative_input_ids, attention_mask=negative_attention_mask).last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Normalize embeddings\n",
        "        query_emb = F.normalize(query_emb, p=2, dim=1)\n",
        "        positive_emb = F.normalize(positive_emb, p=2, dim=1)\n",
        "        negative_emb = F.normalize(negative_emb, p=2, dim=1)\n",
        "\n",
        "        # Compute contrastive loss\n",
        "        loss = contrastive_loss(query_emb, positive_emb, negative_emb)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{EPOCHS}, Step {step}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "# Save the Model\n",
        "os.makedirs(\"fine_tuned_model\", exist_ok=True)\n",
        "model.encoder.save_pretrained(\"fine_tuned_model\")\n",
        "torch.save(model.classification_head.state_dict(), \"fine_tuned_model/classification_head.pt\")\n",
        "torch.save(model.ranking_head.state_dict(), \"fine_tuned_model/ranking_head.pt\")\n",
        "\n",
        "print(\"Fine-tuning complete. Model saved.\")\n"
      ]
    }
  ]
}
